model:
  d_model: 384            # smaller than 512 for M4
  num_heads: 6            # 6 * 64 = 384, cleanly divisible
  d_ff: 1152              # 3 * d_model, rounded to nearest multiple of 64
  num_layers: 4
  vocab_size: 10000
  context_length: 256
  rope_theta: 10000.0

optimizer:
  learning_rate_max: 0.001
  learning_rate_min: 0.0001
  warmup_iters: 100
  cosine_iters: 5000        # same as total training steps
  weight_decay: 0.01

training:
  batch_size: 32            # 32 × 5000 × 256 = 40,960,000 tokens
  max_iters: 5000
  log_every: 50
  val_every: 500
  checkpoint_path: "./cs336_basics/checkpoints/m4_model.pt"
  device: "mps"

dataset:
  input_path: "./data/TinyStoriesV2-GPT4-valid.txt"
  # input_path: "./data/TinyStories-train.txt"
  train_path: "./cs336_basics/dataset/train_m4.bin"
  val_path: "./cs336_basics/dataset/val_m4.bin"
  vocab_out: "./cs336_basics/dataset/vocab_m4.npy"
  merges_out: "./cs336_basics/dataset/merges_m4.npy"
  special_tokens: ["<|endoftext|>"]  

# "<PAD>", "<UNK>", "<BOS>", "<EOS>"