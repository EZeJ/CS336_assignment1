{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa86b50",
   "metadata": {},
   "source": [
    "# 2 Byte-Pair Encoding Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2306c63",
   "metadata": {},
   "source": [
    "## 2.1 The Unicode Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3c8f5",
   "metadata": {},
   "source": [
    "Unicode is a text encoding standard that maps characters to integer code points. As of Unicode 16.0 (released\n",
    "in September 2024), the standard defines 154,998 characters across 168 scripts. For example, the character\n",
    "“s” has the code point 115 (typically notated as U+0073, where U+ is a conventional prefix and 0073 is 115 in\n",
    "hexadecimal), and the character “牛” has the code point 29275. In Python, you can use the ord() function\n",
    "to convert a single Unicode character into its integer representation. The chr() function converts an integer\n",
    "Unicode code point into a string with the corresponding character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99acc777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7310c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e83d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9c0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaae5d5",
   "metadata": {},
   "source": [
    "## 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b0d0c",
   "metadata": {},
   "source": [
    "While the Unicode standard defines a mapping from characters to code points (integers), it’s impractical to\n",
    "train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around\n",
    "150K items) and sparse (since many characters are quite rare). Instead, we’ll use a Unicode encoding, which\n",
    "converts a Unicode character into a sequence of bytes. The Unicode standard itself defines three encodings:\n",
    "UTF-8, UTF-16, and UTF-32, with UTF-8 being the dominant encoding for the Internet (more than 98%\n",
    "of all webpages).\n",
    "To encode a Unicode string into UTF-8, we can use the encode() function in Python. To access the\n",
    "underlying byte values for a Python bytes object, we can iterate over it (e.g., call list()). Finally, we can\n",
    "use the decode() function to decode a UTF-8 byte string into a Unicode string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c8b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"hello! こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ae7b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e67c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "utf16_encoded = test_string.encode(\"utf-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6248485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n"
     ]
    }
   ],
   "source": [
    "print(utf8_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10249b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n"
     ]
    }
   ],
   "source": [
    "print(utf16_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817d3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(utf8_encoded)\n",
    "# what does this do?\n",
    "# it converts the utf8_encoded string to a list of bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c149058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7e497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a0c7f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0775e6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894e0d1",
   "metadata": {},
   "source": [
    "Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into\n",
    "a Unicode string. Why is this function incorrect? Provide an example of an input byte string\n",
    "that yields incorrect results.\n",
    "\n",
    "```python\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    ">>> decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n",
    "'hello'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f0c1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5b075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9704fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decode_utf8_bytes_to_str_wrongode_utf8 = \"hello! こんにちは!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3341b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_decode_utf8_bytes_to_str_wrongode_utf8\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xe3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(test_decode_utf8_bytes_to_str_wrongode_utf8.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d65f21c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'h',\n",
       " b'e',\n",
       " b'l',\n",
       " b'l',\n",
       " b'o',\n",
       " b'!',\n",
       " b' ',\n",
       " b'\\xe3',\n",
       " b'\\x81',\n",
       " b'\\x93',\n",
       " b'\\xe3',\n",
       " b'\\x82',\n",
       " b'\\x93',\n",
       " b'\\xe3',\n",
       " b'\\x81',\n",
       " b'\\xab',\n",
       " b'\\xe3',\n",
       " b'\\x81',\n",
       " b'\\xa1',\n",
       " b'\\xe3',\n",
       " b'\\x81',\n",
       " b'\\xaf',\n",
       " b'!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bytes([b]) for b in test_decode_utf8_bytes_to_str_wrongode_utf8.encode(\"utf-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6727ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 33,\n",
       " 32,\n",
       " 227,\n",
       " 129,\n",
       " 147,\n",
       " 227,\n",
       " 130,\n",
       " 147,\n",
       " 227,\n",
       " 129,\n",
       " 171,\n",
       " 227,\n",
       " 129,\n",
       " 161,\n",
       " 227,\n",
       " 129,\n",
       " 175,\n",
       " 33]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[b for b in test_decode_utf8_bytes_to_str_wrongode_utf8.encode(\"utf-8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88cc8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_correct(bytestring: bytes):\n",
    "    return \"\".join([b.decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1efc1063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decode_utf8_bytes_to_str_wrongode_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09541e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_correct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_decode_utf8_bytes_to_str_wrongode_utf8\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_correct\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_correct\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_correct(test_decode_utf8_bytes_to_str_wrongode_utf8.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12ca00",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "\n",
    "❌ What’s wrong with this function?\n",
    "```\n",
    "\t•\tIt decodes one byte at a time, as if each byte corresponds to an independent UTF-8 character.\n",
    "\t•\tBut UTF-8 is a variable-length encoding, where:\n",
    "\t•\tASCII characters → 1 byte\n",
    "\t•\tOther Unicode characters → 2 to 4 bytes\n",
    "\n",
    "Decoding each byte separately breaks multi-byte sequences, which causes:\n",
    "\t1.\tUnicodeDecodeError (if the byte is not a valid standalone character), or\n",
    "\t2.\tCorrupted output (if decoding doesn’t throw an error but yields incorrect characters)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b78d62",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135ca5c",
   "metadata": {},
   "source": [
    "The BPE tokenizer training procedure consists of three main steps.\n",
    "\n",
    "Vocabulary initialization The tokenizer vocabulary is a one-to-one mapping from bytestring token to integer ID. Since we’re training a byte-level BPE tokenizer, our initial vocabulary is simply the set of all bytes. Since there are 256 possible byte values, our initial vocabulary is of size 256.\n",
    "\n",
    "Pre-tokenization Once you have a vocabulary, you could, in principle, count how often bytes occur next to each other in your text and begin merging them starting with the most frequent pair of bytes. However, this is quite computationally expensive, since we’d have to go take a full pass over the corpus each time we merge. In addition, directly merging bytes across the corpus may result in tokens that differ only in punctuation (e.g., dog! vs. dog.). These tokens would get completely different token IDs, even though they are likely to have high semantic similarity (since they differ only in punctuation).\n",
    "\n",
    "To avoid this, we pre-tokenize the corpus. You can think of this as a coarse-grained tokenization over the corpus that helps us count how often pairs of characters appear. For example, the word 'text' might be a pre-token that appears 10 times. In this case, when we count how often the characters ‘t’ and ‘e’ appear next to each other, we will see that the word ‘text’ has ‘t’ and ‘e’ adjacent and we can increment their count by 10 instead of looking through the corpus. Since we’re training a byte-level BPE model, each pre-token is represented as a sequence of UTF-8 bytes.\n",
    "\n",
    "The original BPE implementation of Sennrich et al. [2016] pre-tokenizes by simply splitting on whitespace (i.e., s.split(\" \")). In contrast, we’ll use a regex-based pre-tokenizer (used by GPT-2; Radford et al., 2019) from github.com/openai/tiktoken/pull/234/files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04bd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861c3ae",
   "metadata": {},
   "source": [
    "## Explanation of Regex Pattern `PAT`\n",
    "\n",
    "### Code\n",
    "```python\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```\n",
    "\n",
    "### What it Does\n",
    "This regex pattern is used to **tokenize text**, often for NLP applications like language models.\n",
    "\n",
    "It uses the `regex` module (not `re`) to support Unicode properties like `\\p{L}`.\n",
    "\n",
    "### Breakdown of the Pattern\n",
    "\n",
    "#### `'(?:[sdmt]|ll|ve|re)`\n",
    "- Matches English **contractions**:\n",
    "  - `'s`, `'d`, `'m`, `'t`, `'ll`, `'ve`, `'re`\n",
    "- `(?:...)` is a **non-capturing group**.\n",
    "\n",
    "#### ` ?\\p{L}+`\n",
    "- Matches optional space followed by one or more **letters** (from any language).\n",
    "- `\\p{L}` = any Unicode letter.\n",
    "\n",
    "#### ` ?\\p{N}+`\n",
    "- Matches optional space followed by one or more **numbers**.\n",
    "- `\\p{N}` = any Unicode numeric digit.\n",
    "\n",
    "#### ` ?[^\\s\\p{L}\\p{N}]+`\n",
    "- Matches optional space followed by one or more **symbols or punctuation**.\n",
    "- It excludes whitespace, letters, and numbers.\n",
    "\n",
    "#### `\\s+(?!\\S)`\n",
    "- Matches **trailing whitespace**.\n",
    "- Negative lookahead `(?!\\S)` ensures it’s not followed by any non-whitespace character.\n",
    "\n",
    "#### `\\s+`\n",
    "- Matches **any other whitespace**.\n",
    "\n",
    "### Usage Example (with `regex` module)\n",
    "```python\n",
    "import regex\n",
    "text = \"Here's an example: 42 tokens, maybe?\"\n",
    "tokens = regex.findall(PAT, text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "- This is a **Unicode-aware tokenizer regex**.\n",
    "- Useful for processing:\n",
    "  - Words\n",
    "  - Numbers\n",
    "  - Punctuation\n",
    "  - Contractions\n",
    "  - Whitespace\n",
    "- Designed for tasks like LLM training or text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af14ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd4436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', \"'s\", ' an', ' example', ':', ' 42', ' tokens', ',', ' maybe', '?', ' 😂']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here's an example: 42 tokens, maybe? 😂\"\n",
    "tokens = re.findall(PAT, text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65d5d8",
   "metadata": {},
   "source": [
    "When using it in your code, however, you should use re.finditer to avoid storing the pre-tokenized words as you construct your mapping from pre-tokens to their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66651753",
   "metadata": {},
   "source": [
    "## Explanation: Why Use `re.finditer` Instead of `re.findall`\n",
    "\n",
    "### Context\n",
    "\n",
    "You have a regex pattern (e.g., `PAT`) for tokenizing text, and you're building a mapping from each **pre-token** to its **count** (like a frequency dictionary).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Difference Between `findall` and `finditer`\n",
    "\n",
    "| Method        | Description                                                                 | Memory Usage        |\n",
    "|---------------|-----------------------------------------------------------------------------|---------------------|\n",
    "| `re.findall()`| Returns a list of all matches as strings                                    | **Higher** (loads all matches) |\n",
    "| `re.finditer()`| Returns an **iterator** yielding match objects one at a time               | **Lower** (streaming) |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Prefer `finditer`\n",
    "\n",
    "- If you're just going to **count** tokens, you don't need to **store** the entire list.\n",
    "- `re.finditer` lets you:\n",
    "  - Stream matches one-by-one\n",
    "  - Avoid unnecessary memory usage\n",
    "  - Work better with large datasets\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "import regex  # Must use 'regex' instead of 're' for \\p{} support\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "text = \"Here's an example: 42 tokens, maybe?\"\n",
    "\n",
    "token_counts = defaultdict(int)\n",
    "\n",
    "for match in regex.finditer(PAT, text):\n",
    "    token = match.group()\n",
    "    token_counts[token] += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Use `re.finditer()` to **avoid storing all tokens** in memory.\n",
    "- Especially useful when **constructing token frequency maps**.\n",
    "- Improves **performance and scalability**.\n",
    "\n",
    "```python\n",
    "# Bad (memory-heavy)\n",
    "tokens = regex.findall(PAT, text)\n",
    "for token in tokens:\n",
    "    token_counts[token] += 1\n",
    "\n",
    "# Good (efficient)\n",
    "for match in regex.finditer(PAT, text):\n",
    "    token = match.group()\n",
    "    token_counts[token] += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e051319d",
   "metadata": {},
   "source": [
    "### Answer: Pretoken Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87943863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_text_with_pre_tokenize(text):\n",
    "    '''\n",
    "    Pre-tokenize the text using regex to match tokens.\n",
    "    This function uses a regex pattern to find tokens in the text.\n",
    "    It returns a dictionary with tokens as keys and their counts as values.\n",
    "    '''\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    token_counts = defaultdict(int)\n",
    "    # Good (efficient)\n",
    "    for match in re.finditer(PAT, text):\n",
    "        token = match.group()\n",
    "        token_counts[token] += 1\n",
    "    return token_counts\n",
    "\n",
    "\n",
    "def convert_dict_to_list(tokens_counts):\n",
    "    '''\n",
    "    Convert the dictionary of token counts to a list of tuples.\n",
    "    Each tuple contains a token and its count.\n",
    "    '''\n",
    "    return list(tokens_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "079d4ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'the': 1, ' cat': 1, ' in': 1, ' the': 1, ' hat': 1})\n",
      "defaultdict(<class 'int'>, {b'the': 1, b' cat': 1, b' in': 1, b' the': 1, b' hat': 1})\n"
     ]
    }
   ],
   "source": [
    "example_text = \"the cat in the hat\"\n",
    "token_counts,token_counts_in_UTF8 = process_text_with_pre_tokenize(example_text)\n",
    "print(token_counts)\n",
    "print(token_counts_in_UTF8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29ed9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ' cat', ' in', ' the', ' hat']\n",
      "[b'the', b' cat', b' in', b' the', b' hat']\n"
     ]
    }
   ],
   "source": [
    "print(convert_dict_to_list(token_counts))\n",
    "print(convert_dict_to_list(token_counts_in_UTF8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d2d30dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(convert_dict_to_list(token_counts_in_UTF8)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9450d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utf8_to_int(tokens_representation):\n",
    "    if isinstance(tokens_representation[0], str):\n",
    "        print(\"Converting str to int\")\n",
    "        return [list(map(ord, string)) for string in tokens_representation]\n",
    "    if isinstance(tokens_representation[0], bytes):\n",
    "        print(\"Converting bytes to int\")\n",
    "        return [list(bytes_item) for bytes_item in tokens_representation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b20c446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting bytes to int\n",
      "Converting str to int\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[116, 104, 101],\n",
       " [32, 99, 97, 116],\n",
       " [32, 105, 110],\n",
       " [32, 116, 104, 101],\n",
       " [32, 104, 97, 116]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_utf8_to_int(convert_dict_to_list(token_counts_in_UTF8))\n",
    "convert_utf8_to_int(convert_dict_to_list(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6410d",
   "metadata": {},
   "source": [
    "### Compute BPE \n",
    "merges Now that we’ve converted our input text into pre-tokens and represented each pre-token as a sequence of UTF-8 bytes, we can compute the BPE merges (i.e., train the BPE tokenizer). At a high level, the BPE algorithm iteratively counts every pair of bytes and identifies the pair with the highest frequency (“A”, “B”). Every occurrence of this most frequent pair (“A”, “B”) is then merged, i.e., replaced with a new token “AB”. This new merged token is added to our vocabulary; as a result, the final vocabulary after BPE training is the size of the initial vocabulary (256 in our case), plus the number of BPE merge operations performed during training. For eﬀiciency during BPE training, we do not consider pairs that cross pre-token boundaries. 2 When computing merges, deterministically break ties in pair frequency by preferring the lexicographically greater pair. For example, if the pairs (“A”, “B”), (“A”, “C”), (“B”, “ZZ”), and (“BA”, “A”) all have the highest frequency, we’d merge (“BA”, “A”):\n",
    "\n",
    ">>> max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")]) ('BA', 'A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dda55",
   "metadata": {},
   "source": [
    "## Explanation: \"Deterministically break ties in pair frequency by preferring the lexicographically greater pair\"\n",
    "\n",
    "### Context\n",
    "This typically applies in **Byte Pair Encoding (BPE)** or similar tokenization algorithms, where you:\n",
    "1. Count how often each pair of symbols appears.\n",
    "2. Merge the most frequent pair.\n",
    "3. Repeat.\n",
    "\n",
    "Sometimes, **two or more pairs** have the **same frequency**. The algorithm needs a way to choose **which one to merge**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔑 Key Terms\n",
    "\n",
    "- **Tie in frequency**: Two symbol pairs occur the same number of times.\n",
    "- **Deterministically**: Always make the same choice given the same input (no randomness).\n",
    "- **Lexicographically greater**: Think of dictionary order — `'z' > 'a'`, `'dog' > 'cat'`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 What It Means\n",
    "\n",
    "> If multiple symbol pairs have the same frequency:\n",
    "> → Choose the one that comes **later in alphabetical order**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Example\n",
    "\n",
    "Assume these are the most frequent pairs with the same frequency:\n",
    "\n",
    "```\n",
    "('th', 10)\n",
    "('he', 10)\n",
    "('in', 10)\n",
    "```\n",
    "\n",
    "To break the tie:\n",
    "- Sort lexicographically:\n",
    "  ```\n",
    "  'th' < 'in' < 'he'  ❌ (wrong)\n",
    "  Actually: 'in' < 'he' < 'th' ✅\n",
    "  ```\n",
    "- Pick the **lexicographically greatest**:\n",
    "  → `'th'`\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Why It’s Important\n",
    "\n",
    "- Ensures **consistency** in training and inference.\n",
    "- Prevents randomness that could lead to mismatched tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Summary\n",
    "\n",
    "> When two symbol pairs are tied in frequency, merge the one that is **alphabetically last**.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "496d6b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "332f4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_ties_during_merge_by_lexicographically(list_of_tuples):\n",
    "    '''\n",
    "    Break ties during merge by lexicographically sorting the tuples.\n",
    "    This function return the maximum tuple based on the first element.\n",
    "    '''\n",
    "    return max(list_of_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58757e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BA', 'A')\n"
     ]
    }
   ],
   "source": [
    "test_of_vocab_pairs = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")]\n",
    "print(break_ties_during_merge_by_lexicographically(test_of_vocab_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d83e0",
   "metadata": {},
   "source": [
    "### Special tokens\n",
    "Often, some strings (e.g., <|endoftext|>) are used to encode metadata (e.g., boundaries between documents). When encoding text, it’s often desirable to treat some strings as “special tokens” that should never be split into multiple tokens (i.e., will always be preserved as a single token). For example, the end-of-sequence string <|endoftext|> should always be preserved as a single token (i.e., a single integer ID), so we know when to stop generating from the language model. These special tokens must be added to the vocabulary, so they have a corresponding fixed token ID.\n",
    "\n",
    "Algorithm 1 of Sennrich et al. [2016] contains an ineﬀicient implementation of BPE tokenizer training (essentially following the steps that we outlined above). As a first exercise, it may be useful to implement and test this function to test your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f20d9a",
   "metadata": {},
   "source": [
    "## TODO: Need to implement this function, for now, it's unclear how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cddee",
   "metadata": {},
   "source": [
    "### Algorithm 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f809d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_ag1 = \"\"\"\n",
    "    low low low low low\n",
    "    lower lower widest widest widest\n",
    "    newest newest newest newest newest newest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83ddff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'\\n   ': 3,\n",
       "             ' low': 5,\n",
       "             ' lower': 2,\n",
       "             ' widest': 3,\n",
       "             ' newest': 6,\n",
       "             '\\n': 1})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text_with_pre_tokenize(example_text_ag1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a656e",
   "metadata": {},
   "source": [
    "## 🤔 Should We Keep Whitespace Matches in BPE Tokenizer?\n",
    "\n",
    "### 📌 Regex in Question\n",
    "```python\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```\n",
    "\n",
    "### 🧠 Observation\n",
    "This pattern **intentionally matches leading spaces**:\n",
    "- `' example'`\n",
    "- `' cat'`\n",
    "- `'\\n'`\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Is This Normal in BPE Tokenizers?\n",
    "\n",
    "**Yes**, this is **normal and intentional** in many BPE tokenizers, especially in models like:\n",
    "- **GPT-2 / GPT-3 / GPT-4**\n",
    "- **T5**\n",
    "- **RoBERTa**\n",
    "\n",
    "They treat **whitespace as meaningful**:\n",
    "- Leading spaces (e.g., `' cat'`) are **part of the token**.\n",
    "- This helps capture context and word boundaries **without needing a special separator**.\n",
    "- For example:\n",
    "  - `'cat'` and `' cat'` are different tokens.\n",
    "  - `'Hello\\nWorld'` keeps the newline to preserve formatting.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Why Not Remove Whitespace?\n",
    "\n",
    "Removing whitespace would:\n",
    "- Break token alignment between training and inference.\n",
    "- Change the meaning of tokens and mess up pretraining statistics.\n",
    "- Lose valuable structure (e.g., indents, newlines, sentence spacing).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ When Should You Remove Whitespace?\n",
    "\n",
    "Only consider removing whitespace:\n",
    "- If you are **preprocessing raw text** for **custom tokenization**.\n",
    "- If your tokenizer is **not whitespace-sensitive** (rare).\n",
    "- If you're using a **character-level model** or models that handle spacing differently.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Summary\n",
    "\n",
    "| 🔍 Behavior                     | ✅ Keep Whitespace |\n",
    "|-------------------------------|--------------------|\n",
    "| GPT-style BPE tokenization    | Yes                |\n",
    "| Word-boundary sensitive model | Yes                |\n",
    "| Character-level model         | Maybe not          |\n",
    "| Custom cleaner/tokenizer      | Optional           |\n",
    "\n",
    "> ✅ **Final Answer**: **Yes**, keeping leading whitespace is **normal** and **recommended** for standard BPE tokenization like GPT-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25732ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_with_pre_tokenize(text, PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"):\n",
    "    '''\n",
    "    Pre-tokenize the text using regex to match tokens.\n",
    "    This function uses a regex pattern to find tokens in the text.\n",
    "    It returns a dictionary with tuple of characters as keys and their counts as values.\n",
    "    '''\n",
    "    token_counts = defaultdict(int)\n",
    "    for match in re.finditer(PAT, text):\n",
    "        token = match.group()\n",
    "        char_tuple = tuple(token)  # Convert string token to tuple of characters\n",
    "        token_counts[char_tuple] += 1\n",
    "    return dict(token_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012f7b5",
   "metadata": {},
   "source": [
    "Great question!\n",
    "\n",
    "In this line:\n",
    "\n",
    "return dict(token_counts)\n",
    "\n",
    "You’re converting the defaultdict to a regular dict. Here’s why this is useful (and sometimes necessary):\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 Reason\n",
    "\n",
    "defaultdict is a subclass of dict, but it has extra behavior: when you access a missing key, it automatically creates it with a default value (like 0 for defaultdict(int)).\n",
    "\n",
    "This can be dangerous or undesired in some contexts, such as:\n",
    "\t•\tSerialization (e.g., saving to JSON) — defaultdict can’t be directly serialized.\n",
    "\t•\tDebugging or displaying — printing defaultdict might be confusing due to its type.\n",
    "\t•\tReturning from a function — to prevent accidental modification or side effects when callers use [] indexing.\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ So, converting to dict:\n",
    "\n",
    "return dict(token_counts)\n",
    "\n",
    "\t•\tMakes the return value a plain Python dictionary (dict).\n",
    "\t•\tRemoves the automatic “default value” behavior.\n",
    "\t•\tIs safer and more portable.\n",
    "\n",
    "⸻\n",
    "\n",
    "🔁 Example\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(int)\n",
    "d['a'] += 1\n",
    "print(d['b'])  # Outputs 0 and adds 'b' to the dict\n",
    "\n",
    "d2 = dict(d)\n",
    "print(d2['c'])  # KeyError!\n",
    "\n",
    "So dict(token_counts) ensures no unexpected key creation occurs later.\n",
    "\n",
    "⸻\n",
    "\n",
    "👇 TL;DR\n",
    "\n",
    "We use dict(...) at return time to:\n",
    "\t•\tStrip away the special behavior of defaultdict\n",
    "\t•\tMake the function output predictable and compatible with other tools and libraries\n",
    "\n",
    "Let me know if you’d like to return a defaultdict instead, or if you want examples of why it matters in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "089ef66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_ag1 = \"\"\"\n",
    "    low low low low low\n",
    "    lower lower widest widest widest\n",
    "    newest newest newest newest newest newest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de853bca",
   "metadata": {},
   "source": [
    "Test regrex pattern for the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "717fc2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('l', 'o', 'w'): 5,\n",
       " ('l', 'o', 'w', 'e', 'r'): 2,\n",
       " ('w', 'i', 'd', 'e', 's', 't'): 3,\n",
       " ('n', 'e', 'w', 'e', 's', 't'): 6}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = process_text_with_pre_tokenize(example_text_ag1, r'\\b[a-zA-Z]+\\b')\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644b258",
   "metadata": {},
   "source": [
    "### Merges \n",
    "We first look at every successive pair of bytes and sum the frequency of the words where they appear {lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}. The pair ('es') and ('st') are tied, so we take the lexicographically greater pair, ('st'). We would then merge the pre-tokens so that we end up with {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}.\n",
    "\n",
    "In the second round, we see that (e, st) is the most common pair (with a count of 9) and we would merge into {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,est): 3, (n,e,w,est): 6}. Continuing this, the sequence of merges we get in the end will be ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e', 'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r'].\n",
    "\n",
    "If we take 6 merges, we have ['s t', 'e st', 'o w', 'l ow', 'w est', 'n e'] and our vocabulary elements would be [<|endoftext|>, [...256 BYTE CHARS], st, est, ow, low, west, ne].\n",
    "\n",
    "With this vocabulary and set of merges, the word newest would tokenize as [ne, west]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c625f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
